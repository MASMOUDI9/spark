import java.util.Arrays;
import org.apache.spark.SparkConf;
import org.apache.spark.streaming.Durations;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import scala.Tuple2;
import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;
import org.apache.spark.sql.types.StructType;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.streaming.OutputMode;
import org.apache.spark.sql.streaming.StreamingQuery;
import org.apache.spark.sql.types.DataType;
import org.apache.spark.sql.types.DataTypes;

public class SparkStreaming {
	private static final Logger logger = LogManager.getLogger(SparkStreaming.class);

    public static void main(String[] args) throws InterruptedException {
        String[] jars = {System.getenv("JARS")};
        SparkConf conf = new SparkConf()
                .setAppName("SparkStreaming")
                .setMaster("local[*]")
                .setSparkHome(System.getenv("SPARK_Home"))
                .setJars(jars);
        JavaStreamingContext jssc = new JavaStreamingContext(conf,Durations.seconds(1));
        JavaDStream<String> stream = jssc.receiverStream(new MyReceiver("books.csv"));
        StructType schema = new StructType()
        		.add("name", DataTypes.StringType)
        		.add("author", DataTypes.StringType)
        		.add("author", DataTypes.FloatType)
        		.add("reviews", DataTypes.IntegerType)
        		.add("Price", DataTypes.IntegerType)
        		.add("year", DataTypes.IntegerType)
        		.add("genre", DataTypes.StringType);
        stream.flatMap(s-> Arrays.asList(s.split(",")).iterator())
        .print();        jssc.start();
        jssc.awaitTermination();
    }

}
