import java.util.Arrays;
import org.apache.spark.SparkConf;
import org.apache.spark.streaming.Durations;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import scala.Tuple2;
public class SparkStreaming {

    public static void main(String[] args) throws InterruptedException {
        String[] jars = {System.getenv("JARS")};
        SparkConf conf = new SparkConf()
                .setAppName("SparkStreaming")
                .setMaster("local[*]")
                .setSparkHome(System.getenv("SPARK_Home"))
                .setJars(jars);
        JavaStreamingContext jssc = new JavaStreamingContext(conf,Durations.seconds(3));
        JavaDStream<String> stream = jssc.receiverStream(new MyReceiver("cars.csv"));
        stream.flatMap(s-> Arrays.asList(s.split(",")).iterator())
                .mapToPair(word->new Tuple2<>(word, 1))
                .reduceByKey((v1, v2) -> v1 + v2)
                .print();
        jssc.start();
        jssc.awaitTermination();
    }

}
