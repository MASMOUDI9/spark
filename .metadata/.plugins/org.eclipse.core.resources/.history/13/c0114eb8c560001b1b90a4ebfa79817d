import java.util.Arrays;
import org.apache.spark.SparkConf;
import org.apache.spark.streaming.Durations;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import scala.Tuple2;
import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;
public class SparkStreaming {
	private static final Logger logger = LogManager.getLogger(SparkStreaming.class);

    public static void main(String[] args) throws InterruptedException {
        String[] jars = {System.getenv("JARS")};
        SparkConf conf = new SparkConf()
                .setAppName("SparkStreaming")
                .setMaster("local[*]")
                .setSparkHome(System.getenv("SPARK_Home"))
                .setJars(jars);
        JavaStreamingContext jssc = new JavaStreamingContext(conf,Durations.seconds(3));
        JavaDStream<String> stream = jssc.receiverStream(new MyReceiver("books.csv"));
        stream.flatMap(s-> Arrays.asList(s.split(",")).iterator())
        	.print();
        jssc.start();
        jssc.awaitTermination();
    }

}
